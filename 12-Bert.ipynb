{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vocab.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TorchVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchVocab(object):\n",
    "\n",
    "    def __init__(self, counter, max_size=None, min_freq=1, specials=['<pad>', '<oov>'],\n",
    "                 vectors=None, unk_init=None, vectors_cache=None):\n",
    "\n",
    "        self.freqs = counter\n",
    "        counter = counter.copy()\n",
    "        min_freq = max(min_freq, 1)\n",
    "\n",
    "        self.itos = list(specials)\n",
    "        # frequencies of special tokens are not counted when building vocabulary\n",
    "        # in frequency order\n",
    "        for tok in specials:\n",
    "            del counter[tok]\n",
    "\n",
    "        max_size = None if max_size is None else max_size + len(self.itos)\n",
    "\n",
    "        # sort by frequency, then alphabetically\n",
    "        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
    "        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        for word, freq in words_and_frequencies:\n",
    "            if freq < min_freq or len(self.itos) == max_size:\n",
    "                break\n",
    "            self.itos.append(word)\n",
    "\n",
    "        # stoi is simply a reverse dict for itos\n",
    "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
    "\n",
    "        self.vectors = None\n",
    "        if vectors is not None:\n",
    "            self.load_vectors(vectors, unk_init=unk_init, cache=vectors_cache)\n",
    "        else:\n",
    "            assert unk_init is None and vectors_cache is None\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if self.freqs != other.freqs:\n",
    "            return False\n",
    "        if self.stoi != other.stoi:\n",
    "            return False\n",
    "        if self.itos != other.itos:\n",
    "            return False\n",
    "        if self.vectors != other.vectors:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def vocab_rerank(self):\n",
    "        self.stoi = {word: i for i, word in enumerate(self.itos)}\n",
    "\n",
    "    def extend(self, v, sort=False):\n",
    "        words = sorted(v.itos) if sort else v.itos\n",
    "        for w in words:\n",
    "            if w not in self.stoi:\n",
    "                self.itos.append(w)\n",
    "                self.stoi[w] = len(self.itos) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(TorchVocab):\n",
    "    def __init__(self, counter, max_size=None, min_freq=1):\n",
    "        self.pad_index = 0\n",
    "        self.unk_index = 1\n",
    "        self.eos_index = 2\n",
    "        self.sos_index = 3\n",
    "        self.mask_index = 4\n",
    "        super().__init__(counter, specials=[\"<pad>\", \"<unk>\", \"<eos>\", \"<sos>\", \"<mask>\"],\n",
    "                         max_size=max_size, min_freq=min_freq)\n",
    "\n",
    "    def to_seq(self, sentece, seq_len, with_eos=False, with_sos=False) -> list:\n",
    "        pass\n",
    "\n",
    "    def from_seq(self, seq, join=False, with_pad=False):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'Vocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def save_vocab(self, vocab_path):\n",
    "        with open(vocab_path, \"wb\") as f:\n",
    "            pickle.dump(self, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordVocab(Vocab):\n",
    "    def __init__(self, texts, max_size=None, min_freq=1):\n",
    "        print(\"Building Vocab\")\n",
    "        counter = Counter()\n",
    "        for line in tqdm.tqdm(texts):\n",
    "            if isinstance(line, list):\n",
    "                words = line\n",
    "            else:\n",
    "                words = line.replace(\"\\n\", \"\").replace(\"\\t\", \"\").split()\n",
    "\n",
    "            for word in words:\n",
    "                counter[word] += 1\n",
    "        super().__init__(counter, max_size=max_size, min_freq=min_freq)\n",
    "\n",
    "    def to_seq(self, sentence, seq_len=None, with_eos=False, with_sos=False, with_len=False):\n",
    "        if isinstance(sentence, str):\n",
    "            sentence = sentence.split()\n",
    "\n",
    "        seq = [self.stoi.get(word, self.unk_index) for word in sentence]\n",
    "\n",
    "        if with_eos:\n",
    "            seq += [self.eos_index]  # this would be index 1\n",
    "        if with_sos:\n",
    "            seq = [self.sos_index] + seq\n",
    "\n",
    "        origin_seq_len = len(seq)\n",
    "\n",
    "        if seq_len is None:\n",
    "            pass\n",
    "        elif len(seq) <= seq_len:\n",
    "            seq += [self.pad_index for _ in range(seq_len - len(seq))]\n",
    "        else:\n",
    "            seq = seq[:seq_len]\n",
    "\n",
    "        return (seq, origin_seq_len) if with_len else seq\n",
    "\n",
    "    def from_seq(self, seq, join=False, with_pad=False):\n",
    "        words = [self.itos[idx]\n",
    "                 if idx < len(self.itos)\n",
    "                 else \"<%d>\" % idx\n",
    "                 for idx in seq\n",
    "                 if not with_pad or idx != self.pad_index]\n",
    "\n",
    "        return \" \".join(words) if join else words\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'WordVocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 544.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Vocab\n",
      "VOCAB SIZE: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_path=args.train_dataset\n",
    "with open(corpus_path, \"r\") as f:\n",
    "    vocab = WordVocab(f, max_size=None, min_freq=1)\n",
    "\n",
    "print(\"VOCAB SIZE:\", len(vocab))\n",
    "vocab.save_vocab(args.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=WordVocab.load_vocab(args.vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset: 2it [00:00, 871.00it/s]\n",
      "Loading Dataset: 2it [00:00, 5551.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Train Dataset data/corpus.small\n",
      "Loading Test Dataset data/corpus.small\n",
      "Creating Dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Train Dataset\", args.train_dataset)\n",
    "train_dataset = BERTDataset(args.train_dataset, vocab, seq_len=args.seq_len, corpus_lines=args.corpus_lines)\n",
    "\n",
    "print(\"Loading Test Dataset\", args.test_dataset)\n",
    "test_dataset = BERTDataset(args.test_dataset, vocab,\n",
    "                           seq_len=args.seq_len) if args.test_dataset is not None else None\n",
    "\n",
    "print(\"Creating Dataloader\")\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=args.num_workers)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=args.batch_size, num_workers=args.num_workers) \\\n",
    "    if test_dataset is not None else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "\n",
    "class BERTTrainer:\n",
    "    \"\"\"\n",
    "    BERTTrainer make the pretrained BERT model with two LM training method.\n",
    "\n",
    "        1. Masked Language Model : 3.3.1 Task #1: Masked LM\n",
    "        2. Next Sentence prediction : 3.3.2 Task #2: Next Sentence Prediction\n",
    "\n",
    "    please check the details on README.md with simple example.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert: BERT, vocab_size: int,\n",
    "                 train_dataloader: DataLoader, test_dataloader: DataLoader = None,\n",
    "                 lr: float = 1e-4, betas=(0.9, 0.999), weight_decay: float = 0.01,\n",
    "                 with_cuda: bool = True, log_freq: int = 10):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which you want to train\n",
    "        :param vocab_size: total word vocab size\n",
    "        :param train_dataloader: train dataset data loader\n",
    "        :param test_dataloader: test dataset data loader [can be None]\n",
    "        :param lr: learning rate of optimizer\n",
    "        :param betas: Adam optimizer betas\n",
    "        :param weight_decay: Adam optimizer weight decay param\n",
    "        :param with_cuda: traning with cuda\n",
    "        :param log_freq: logging frequency of the batch iteration\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup cuda device for BERT training, argument -c, --cuda should be true\n",
    "        cuda_condition = torch.cuda.is_available() and with_cuda\n",
    "        self.device = torch.device(\"cuda:0\" if cuda_condition else \"cpu\")\n",
    "\n",
    "        # This BERT model will be saved every epoch\n",
    "        self.bert = bert\n",
    "        # Initialize the BERT Language Model, with BERT model\n",
    "        self.model = BERTLM(bert, vocab_size).to(self.device)\n",
    "\n",
    "        # Distributed GPU training if CUDA can detect more than 1 GPU\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Using %d GPUS for BERT\" % torch.cuda.device_count())\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "\n",
    "        # Setting the train and test data loader\n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "\n",
    "        # Setting the Adam optimizer with hyper-param\n",
    "        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "\n",
    "        # Using Negative Log Likelihood Loss function for predicting the masked_token\n",
    "        self.criterion = nn.NLLLoss(ignore_index=0)\n",
    "\n",
    "        self.log_freq = log_freq\n",
    "\n",
    "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.iteration(epoch, self.train_data)\n",
    "\n",
    "    def test(self, epoch):\n",
    "        self.iteration(epoch, self.test_data, train=False)\n",
    "\n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "       \n",
    "        str_code = \"train\" if train else \"test\"\n",
    "\n",
    "        # Setting the tqdm progress bar\n",
    "        data_iter = tqdm.tqdm(enumerate(data_loader),\n",
    "                              desc=\"EP_%s:%d\" % (str_code, epoch),\n",
    "                              total=len(data_loader),\n",
    "                              bar_format=\"{l_bar}{r_bar}\")\n",
    "\n",
    "        avg_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_element = 0\n",
    "\n",
    "        for i, data in data_iter:\n",
    "            # 0. batch_data will be sent into the device(GPU or cpu)\n",
    "            data = {key: value.to(self.device) for key, value in data.items()}\n",
    "\n",
    "            # 1. forward the next_sentence_prediction and masked_lm model\n",
    "            next_sent_output, mask_lm_output = self.model.forward(data[\"bert_input\"], data[\"segment_label\"])\n",
    "\n",
    "            # 2-1. NLL(negative log likelihood) loss of is_next classification result\n",
    "            next_loss = self.criterion(next_sent_output, data[\"is_next\"])\n",
    "\n",
    "            # 2-2. NLLLoss of predicting masked token word\n",
    "            mask_loss = self.criterion(mask_lm_output.transpose(1, 2), data[\"bert_label\"])\n",
    "\n",
    "            # 2-3. Adding next_loss and mask_loss : 3.4 Pre-training Procedure\n",
    "            loss = next_loss + mask_loss\n",
    "\n",
    "            # 3. backward and optimization only in train\n",
    "            if train:\n",
    "                self.optim.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "\n",
    "            # next sentence prediction accuracy\n",
    "            correct = next_sent_output.argmax(dim=-1).eq(data[\"is_next\"]).sum().item()\n",
    "            avg_loss += loss.item()\n",
    "            total_correct += correct\n",
    "            total_element += data[\"is_next\"].nelement()\n",
    "\n",
    "            post_fix = {\n",
    "                \"epoch\": epoch,\n",
    "                \"iter\": i,\n",
    "                \"avg_loss\": avg_loss / (i + 1),\n",
    "                \"avg_acc\": total_correct / total_element * 100,\n",
    "                \"loss\": loss.item()\n",
    "            }\n",
    "\n",
    "            if i % self.log_freq == 0:\n",
    "                data_iter.write(str(post_fix))\n",
    "\n",
    "        print(\"EP%d_%s, avg_loss=\" % (epoch, str_code), avg_loss / len(data_iter), \"total_acc=\",\n",
    "              total_correct * 100.0 / total_element)\n",
    "\n",
    "    def save(self, epoch, file_path=\"output/bert_trained.model\"):\n",
    "        output_path = file_path + \".ep%d\" % epoch\n",
    "        torch.save(self.bert.cpu(), output_path)\n",
    "        self.bert.to(self.device)\n",
    "        print(\"EP:%d Model Saved on:\" % epoch, output_path)\n",
    "        return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTLM(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Language Model\n",
    "    Next Sentence Prediction Model + Masked Language Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert: BERT, vocab_size):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which should be trained\n",
    "        :param vocab_size: total vocab size for masked_lm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.next_sentence = NextSentencePrediction(self.bert.hidden)\n",
    "        self.mask_lm = MaskedLanguageModel(self.bert.hidden, vocab_size)\n",
    "\n",
    "    def forward(self, x, segment_label):\n",
    "        x = self.bert(x, segment_label)\n",
    "        return self.next_sentence(x), self.mask_lm(x)\n",
    "\n",
    "\n",
    "class NextSentencePrediction(nn.Module):\n",
    "    \"\"\"\n",
    "    2-class classification model : is_next, is_not_next\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden):\n",
    "        \"\"\"\n",
    "        :param hidden: BERT model output size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x[:, 0]))\n",
    "\n",
    "\n",
    "class MaskedLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    predicting origin token from masked input sequence\n",
    "    n-class classification problem, n-class = vocab_size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        \"\"\"\n",
    "        :param hidden: output size of BERT model\n",
    "        :param vocab_size: total vocab size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BERT Trainer\n",
      "Total Parameters: 6327057\n",
      "Training Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0: 100%|| 1/1 [00:00<00:00,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 0, 'avg_loss': 2.986823081970215, 'avg_acc': 0.0, 'loss': 2.986823081970215}\n",
      "EP0_train, avg_loss= 2.986823081970215 total_acc= 0.0\n",
      "EP:0 Model Saved on: data/corpus.small.vocab.ep0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/usr/local/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type BERT. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type BERTEmbedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type TokenEmbedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type PositionalEmbedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type SegmentEmbedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type TransformerBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type MultiHeadedAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type PositionwiseFeedForward. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type GELU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type SublayerConnection. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "EP_test:0: 100%|| 1/1 [00:00<00:00,  7.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 0, 'avg_loss': 4.260068416595459, 'avg_acc': 50.0, 'loss': 4.260068416595459}\n",
      "EP0_test, avg_loss= 4.260068416595459 total_acc= 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:1: 100%|| 1/1 [00:00<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 0, 'avg_loss': 6.681731224060059, 'avg_acc': 50.0, 'loss': 6.681731224060059}\n",
      "EP1_train, avg_loss= 6.681731224060059 total_acc= 50.0\n",
      "EP:1 Model Saved on: data/corpus.small.vocab.ep1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_test:1: 100%|| 1/1 [00:00<00:00,  8.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 0, 'avg_loss': 2.9084110260009766, 'avg_acc': 50.0, 'loss': 2.9084110260009766}\n",
      "EP1_test, avg_loss= 2.9084110260009766 total_acc= 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:2: 100%|| 1/1 [00:00<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 0, 'avg_loss': 9.12502670288086, 'avg_acc': 0.0, 'loss': 9.12502670288086}\n",
      "EP2_train, avg_loss= 9.12502670288086 total_acc= 0.0\n",
      "EP:2 Model Saved on: data/corpus.small.vocab.ep2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:2: 100%|| 1/1 [00:00<00:00,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 0, 'avg_loss': 0.13810335099697113, 'avg_acc': 50.0, 'loss': 0.13810335099697113}\n",
      "EP2_test, avg_loss= 0.13810335099697113 total_acc= 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:3: 100%|| 1/1 [00:00<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 0, 'avg_loss': 2.3999338150024414, 'avg_acc': 50.0, 'loss': 2.3999338150024414}\n",
      "EP3_train, avg_loss= 2.3999338150024414 total_acc= 50.0\n",
      "EP:3 Model Saved on: data/corpus.small.vocab.ep3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:3: 100%|| 1/1 [00:00<00:00,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 0, 'avg_loss': 1.0868234634399414, 'avg_acc': 0.0, 'loss': 1.0868234634399414}\n",
      "EP3_test, avg_loss= 1.0868234634399414 total_acc= 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:4: 100%|| 1/1 [00:00<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 0, 'avg_loss': 3.5171289443969727, 'avg_acc': 50.0, 'loss': 3.5171289443969727}\n",
      "EP4_train, avg_loss= 3.5171289443969727 total_acc= 50.0\n",
      "EP:4 Model Saved on: data/corpus.small.vocab.ep4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:4: 100%|| 1/1 [00:00<00:00,  8.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 0, 'avg_loss': 0.08919079601764679, 'avg_acc': 100.0, 'loss': 0.08919079601764679}\n",
      "EP4_test, avg_loss= 0.08919079601764679 total_acc= 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:5: 100%|| 1/1 [00:00<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'iter': 0, 'avg_loss': 4.4835968017578125, 'avg_acc': 100.0, 'loss': 4.4835968017578125}\n",
      "EP5_train, avg_loss= 4.4835968017578125 total_acc= 100.0\n",
      "EP:5 Model Saved on: data/corpus.small.vocab.ep5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:5: 100%|| 1/1 [00:00<00:00,  9.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'iter': 0, 'avg_loss': 3.861670970916748, 'avg_acc': 50.0, 'loss': 3.861670970916748}\n",
      "EP5_test, avg_loss= 3.861670970916748 total_acc= 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:6: 100%|| 1/1 [00:00<00:00,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'iter': 0, 'avg_loss': 0.00042926802416332066, 'avg_acc': 100.0, 'loss': 0.00042926802416332066}\n",
      "EP6_train, avg_loss= 0.00042926802416332066 total_acc= 100.0\n",
      "EP:6 Model Saved on: data/corpus.small.vocab.ep6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_test:6: 100%|| 1/1 [00:00<00:00,  9.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'iter': 0, 'avg_loss': 0.01838160678744316, 'avg_acc': 0.0, 'loss': 0.01838160678744316}\n",
      "EP6_test, avg_loss= 0.01838160678744316 total_acc= 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:7: 100%|| 1/1 [00:00<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 7, 'iter': 0, 'avg_loss': 1.6109089851379395, 'avg_acc': 50.0, 'loss': 1.6109089851379395}\n",
      "EP7_train, avg_loss= 1.6109089851379395 total_acc= 50.0\n",
      "EP:7 Model Saved on: data/corpus.small.vocab.ep7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_test:7: 100%|| 1/1 [00:00<00:00, 10.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 7, 'iter': 0, 'avg_loss': 3.692484140396118, 'avg_acc': 50.0, 'loss': 3.692484140396118}\n",
      "EP7_test, avg_loss= 3.692484140396118 total_acc= 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:8: 100%|| 1/1 [00:00<00:00,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 8, 'iter': 0, 'avg_loss': 5.374189376831055, 'avg_acc': 50.0, 'loss': 5.374189376831055}\n",
      "EP8_train, avg_loss= 5.374189376831055 total_acc= 50.0\n",
      "EP:8 Model Saved on: data/corpus.small.vocab.ep8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_test:8: 100%|| 1/1 [00:00<00:00, 13.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 8, 'iter': 0, 'avg_loss': 6.1481781005859375, 'avg_acc': 100.0, 'loss': 6.1481781005859375}\n",
      "EP8_test, avg_loss= 6.1481781005859375 total_acc= 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:9: 100%|| 1/1 [00:00<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 9, 'iter': 0, 'avg_loss': 0.0006999903125688434, 'avg_acc': 50.0, 'loss': 0.0006999903125688434}\n",
      "EP9_train, avg_loss= 0.0006999903125688434 total_acc= 50.0\n",
      "EP:9 Model Saved on: data/corpus.small.vocab.ep9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:9: 100%|| 1/1 [00:00<00:00, 11.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 9, 'iter': 0, 'avg_loss': 2.9055590629577637, 'avg_acc': 100.0, 'loss': 2.9055590629577637}\n",
      "EP9_test, avg_loss= 2.9055590629577637 total_acc= 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating BERT Trainer\")\n",
    "trainer = BERTTrainer(bert, len(vocab), train_dataloader=train_data_loader, test_dataloader=test_data_loader,\n",
    "                      lr=args.lr, betas=(args.adam_beta1, args.adam_beta2), weight_decay=args.adam_weight_decay,\n",
    "                      with_cuda=args.with_cuda, log_freq=args.log_freq)\n",
    "\n",
    "print(\"Training Start\")\n",
    "for epoch in range(args.epochs):\n",
    "    trainer.train(epoch)\n",
    "    trainer.save(epoch, args.output_path)\n",
    "\n",
    "    if test_data_loader is not None:\n",
    "        trainer.test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
