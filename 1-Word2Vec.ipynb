{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Estimation of Word Representations in Vector Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embed_in = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.embed_out = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self._init_embed()\n",
    "        \n",
    "        def _init_embed():     \n",
    "            initrange = 0.5 / self.embedding_dim\n",
    "            self.embed_in.weight.data.uniform_(-initrange, initrange)\n",
    "            self.embed_out.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        def forward(self, center, pos, neg):   #  center: batch_size\n",
    "            batch_size = center.size(0)\n",
    "            center = torch.LongTensor(center).cuda()\n",
    "            pos = torch.LongTensor(pog).cuda()\n",
    "            neg = torch.LongTensor(neg).cuda()\n",
    "            \n",
    "            center_embed = self.embed_in(center) # b, embed\n",
    "            pos_embed = self.embed_out(pog)      # b, 2c, embed\n",
    "            neg_embed = self.embed_out(neg)      # b, 2ck, embed\n",
    "            \n",
    "            log_pos = torch.bmm(pos_embed, center_embed.unsqueeze(2)).squeeze()  # b, 2c\n",
    "            log_neg = torch.bmm(neg_mebed, -center_embed.unsqueeze(2)).squeeze()  # b, 2ck\n",
    "            \n",
    "            log_pos = F.logsimoid(log_pos).sum(1)\n",
    "            log_neg = F.logsimoid(log_neg).sum(1)\n",
    "            loss = log_pos + log_neg\n",
    "            return -loss\n",
    "        \n",
    "        def save_embedding(self, idx2word, file):\n",
    "            embedding_1 = self.embed_in.weight.data.cpu().numpy()\n",
    "            embedding_2 = self.embed_out.weight.data.cpu().numpy()\n",
    "            embedding = (embedding_1 + embedding_2)/2\n",
    "            f = open('f_out', 'w')\n",
    "            f.write('%d %d\\n' % (len(idx2word), self.embed_in))\n",
    "            for idx, w in idx2word.items():\n",
    "                e = embedding[idx]\n",
    "                e = ' '.join(map(lambda x: str(x), e))\n",
    "                f.wirte('%d %d\\n' % (w, e))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondac1ed852f726f46e99aa9eda212d43d36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
