{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-587457eb007b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mAttention_NMT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     def __init__(self, source_vocab_size=3000, target_vocab_size=3000, embedding_size=128,\n\u001b[0;32m      3\u001b[0m                  source_length=100, target_length=100, lstm_size=256, batch_size=64):\n\u001b[0;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAttention_NMT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Attention_NMT(nn.Module):\n",
    "    def __init__(self, source_vocab_size=3000, target_vocab_size=3000, embedding_size=128,\n",
    "                 source_length=100, target_length=100, lstm_size=256, batch_size=64):\n",
    "        super(Attention_NMT,self).__init__()\n",
    "        \n",
    "        self.source_embedding =nn.Embedding(source_vocab_size, embedding_size) \n",
    "        self.target_embedding = nn.Embedding(target_vocab_size, embedding_size) \n",
    "        self.encoder = nn.LSTM(input_size=embedding_size, hidden_size=lstm_size, num_layers=1, \n",
    "                               bidirectional=True, batch_first=True)  # 128, 256\n",
    "        self.decoder = nn.LSTM(input_size=embedding_size + 2 * lstm_size, hidden_size=lstm_size, num_layers=1,\n",
    "                               batch_first=True)   # 640, 256\n",
    "        \n",
    "        self.attention_fc_1 = nn.Linear(3 * lstm_size, 3 * lstm_size) # 768, 768\n",
    "        self.attention_fc_2 = nn.Linear(3 * lstm_size, 1) # 768, 1\n",
    "        self.class_fc_1 = nn.Linear(embedding_size + 2 * lstm_size + lstm_size, 2 * lstm_size) # 896, 512\n",
    "        self.class_fc_2 = nn.Linear(2 * lstm_size, target_vocab_size) # 512, 3000\n",
    "        \n",
    "    # input_embedding: 64, 1, 128\n",
    "    # dec_prev_hidden: 2, 64, 256\n",
    "    # enc_output:      64, 100, 512\n",
    "    def attention_forward(self, input_embedding, dec_prev_hidden, enc_output):\n",
    "        '''\n",
    "        query: st-1\n",
    "        key = value = hi\n",
    "        '''\n",
    "        # si-1\n",
    "        prev_dec_h = dec_prev_hidden[0].squeeze().unsqueeze(1).repeat(1, 100, 1) # 64, 1, 256 -> 64, 100, 256\n",
    "        # eij = a(si-1, hj)  通过全连接生成权重\n",
    "        atten_input = torch.cat([prev_dec_h, enc_output], dim=-1) # 64, 100, 256+512=768\n",
    "        # attention全连接层\n",
    "        attention_weights = self.attention_fc_2(F.relu(self.attention_fc_1(atten_input)))  # 64,100,768->64,100,768->64,100,1\n",
    "        # aij = softmax(eij)\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)   # 64, 100, 1\n",
    "        # atten_output = ci = sum(aij * hj)\n",
    "        atten_output = torch.sum(attention_weights * enc_output, dim=1).unsqueeze(1) # 64, 100, 512 -> 64, 512 -> 64, 1, 512 \n",
    "        \n",
    "        dec_lstm_input = torch.cat([input_embedding, atten_output], dim=2)  # 64, 1, 640\n",
    "        dec_output, dec_hidden = self.decoder(dec_lstm_input, dec_prev_hidden) # 64, 1, 512  / 2, 64, 256\n",
    "        return atten_output, dec_output, dec_hidden  # 64, 1, 512 / 64, 1, 512 / 2, 64, 256\n",
    "    \n",
    "    \n",
    "    def forward(self, source_data, target_data, mode = \"train\", is_gpu=True):\n",
    "        source_data_embedding = self.source_embedding(source_data)   # 64, 100, 128\n",
    "        # enc_output: b * length * (2*lstm_size) 返回所有hidden, concat\n",
    "        # enc_hidden：[[h1,h2],[c1,c2]] 返回每个方向最后一个时间步的h和c\n",
    "        enc_output, enc_hidden = self.encoder(source_data_embedding)  # 64, 100, 512  / 2, 64, 256 \n",
    "        # Variable包裹tensor后可以进行反向传播，与tensor无太大区别\n",
    "        self.atten_outputs = Variable(torch.zeros(target_data.shape[0],  \n",
    "                                                  target_data.shape[1], \n",
    "                                                  enc_output.shape[2])) # 64, 100, 512\n",
    "        self.dec_outputs = Variable(torch.zeros(target_data.shape[0], \n",
    "                                                target_data.shape[1], \n",
    "                                                enc_hidden[0].shape[2])) # 64, 100, 512\n",
    "        if is_gpu:\n",
    "            self.atten_outputs = self.atten_outputs.cuda()\n",
    "            self.dec_outputs = self.dec_outputs.cuda()\n",
    "            \n",
    "        if mode==\"train\": \n",
    "            target_data_embedding = self.target_embedding(target_data)  # 64, 100, 128\n",
    "            # 合并最后一个时间步的同向的h和c\n",
    "            dec_prev_hidden = [enc_hidden[0][0].unsqueeze(0), enc_hidden[1][0].unsqueeze(0)]  # 2, 64, 256\n",
    "        \n",
    "            for i in range(100):\n",
    "                input_embedding = target_data_embedding[:, i, :].unsqueeze(1)  # 64, 1, 128\n",
    "                # 64, 1, 512 / 64, 1, 512 / 2, 64, 256\n",
    "                atten_output, dec_output, dec_hidden = self.attention_forward(input_embedding,\n",
    "                                                                              dec_prev_hidden,\n",
    "                                                                              enc_output)  \n",
    "                self.atten_outputs[:, i] = atten_output.squeeze()  # 64, 512\n",
    "                self.dec_outputs[:, i] = dec_output.squeeze()    # 64, 256\n",
    "                dec_prev_hidden = dec_hidden   # 2, 64, 256\n",
    "            # 64, 100, 128 + 64, 100, 512 + 64, 100, 256 = 64, 100, 896\n",
    "            class_input = torch.cat([target_data_embedding, self.atten_outputs, self.dec_outputs], dim=2)  \n",
    "            # 64, 100, 896 -> 64, 100, 512 -> 64, 100, 3000\n",
    "            outs = self.class_fc_2(F.relu(self.class_fc_1(class_input)))\n",
    "        else:\n",
    "            input_embedding = self.target_embedding(target_data)\n",
    "            dec_prev_hidden = [enc_hidden[0][0].unsqueeze(0), enc_hidden[1][0].unsqueeze(0)]\n",
    "            outs = []\n",
    "            for i in range(100):\n",
    "                atten_output, dec_output, dec_hidden = self.attention_forward(input_embedding,\n",
    "                                                                              dec_prev_hidden,\n",
    "                                                                              enc_output)\n",
    "\n",
    "                class_input = torch.cat([input_embedding, atten_output, dec_output], dim=2)\n",
    "                pred = self.class_fc_2(F.relu(self.class_fc_1(class_input)))\n",
    "                pred = torch.argmax(pred, dim=-1)\n",
    "                outs.append(pred.squeeze().cpu().numpy())\n",
    "                dec_prev_hidden = dec_hidden\n",
    "                input_embedding = self.target_embedding(pred)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100, 896])\n",
      "torch.Size([64, 100, 3000])\n",
      "(100, 64)\n"
     ]
    }
   ],
   "source": [
    "model = Attention_NMT()\n",
    "source_data = torch.Tensor(np.zeros([64,100])).long()\n",
    "target_data = torch.Tensor(np.zeros([64,100])).long()\n",
    "preds = model(source_data, target_data,is_gpu=False)\n",
    "print (preds.shape)\n",
    "\n",
    "target_data = torch.Tensor(np.zeros([64, 1])).long()\n",
    "preds = model(source_data, target_data, mode=\"test\", is_gpu=False)\n",
    "print(np.array(preds).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondac1ed852f726f46e99aa9eda212d43d36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
